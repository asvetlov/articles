Title: aiozmq benchmark -- обновление
Labels: aiozmq, benchmark


Вчера написал [измеритель
производительности](http://asvetlov.blogspot.com/2014/04/aiozmq-benchmark.html)
*aiozmq*.

Аж пять тестов, три для *pyzmq* и два для *aiozmq*.

Сегодня захотел посмотреть, насколько правильные цифры они показывают.

Для оценки результатов измерений давным-давно была изобретена
математическая статистика. Имея выборку (результаты нескольких
замеров) вычисляем [среднеквадратическое
отклонение](http://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BE%D1%82%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)
и [доверительный 95%
интервал](http://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B5%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%B2%D0%B0%D0%BB).
Последний более понятен для интерпретации: если **на пальцах** то это тот
диапазон, в который попадает наша величина с вероятностью 95%.

Дело в том что результаты измерений всегда дают *некоторый разброс*, и
нужно оценить **насколько этот разброс велик** (можно ли верить
полученному **среднему значению**).

В *numpy* / *scipy* уже всё есть, математику вспоминать не нужно.

Пример для вычисления **request per second** для списка *data*, каждый
элемент которого содержит время выполнения *count* запросов *RPC*.

    from scipy.stats import norm, tmean, tvar
    from numpy import array

    rps = count / array(data)
    rps_mean = tmean(rps)
    rps_var = tvar(rps)
    low, high = norm.interval(0.95, loc=rps_mean, scale=rps_var**0.5)


## Анализ

И что я увидел? Тесты для *aiozmq* давали просто чудовищный разброс:
*среднеквадратическое отклонение* зашкаливало, а нижняя граница
*доверительного интервала* иногда уходила в минуса.

**Т.е. моим тестам просто нельзя верить!!!**

Несколько раз перепроверив убедился, что проблема именно в тестах а не
в неправильной обработке статистики.

Начал рабираться и довольно быстро обнаружил источник беды: я не
освобождал все ресурсы после запуска теста (не закрывал транспорты и
сервисы *aiozmq*). В результате при каждом следующем запуске тестового
кода *asyncio event loop* был вынужден делать чуть больше
работы. Совсем чуть-чуть, но ведь мы измеряем десятые доли милисекунды
и разница начинает быть заметной.

Быстро поправил и жизнь наладилась.

Плюс [Олег
Нечаев](https://plus.google.com/u/0/104735810233733294937/posts)
заметил, что тест для *aiozmq core* не вполне корректен и занимает на
25% больше времени чем должен.

Первоначальные 3-5 запусков каждого теста были явно недостаточны для сбора
статистики. Из институтских лаб помню, что **выборка** в которой **меньше 30
измерений** -- и не выборка уже а сплошное недоразумение. Увеличил
количество до 100.

Чтобы тесты показывали более стабильные результаты увеличил до 5000
количество *request-reply* для каждого теста.

Время работы измерялки стало напрягать. Добавил запуск тестов [на
нескольких
процессах](https://docs.python.org/3/library/multiprocessing.html).

У меня четыре виртуальных ядра, так что время уменьшилось в ожидаемые 4 раза.


Обновленные тесты:

    (aiozmq)andrew@tiktaalik-3:~/projects/aiozmq (master)$ python benchmarks/simple.py -n 5000 -t 100
    Run tests for 100*5000 iterations: ['aiozmq.rpc', 'core aiozmq', 'single thread raw zmq', 'single thread zmq with poller', 'zmq with threads']
    ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................

    RPS calculated as 95% confidence interval
    Results for aiozmq.rpc
    RPS: 3797: [3589, 4005],	mean: 263.533 μs,	standard deviation 7.252 μs

    Results for core aiozmq
    RPS: 10875: [9806, 11944],	mean: 92.190 μs,	standard deviation 4.842 μs

    Results for single thread raw zmq
    RPS: 20622: [14559, 26684],	mean: 49.840 μs,	standard deviation 9.238 μs

    Results for single thread zmq with poller
    RPS: 18020: [16687, 19353],	mean: 55.572 μs,	standard deviation 2.147 μs

    Results for zmq with threads
    RPS: 19550: [14768, 24331],	mean: 52.101 μs,	standard deviation 7.761 μs

Машинка другая, десктопный *Intel(R) Core(TM) i5-3470 CPU @ 3.20GHz,
8GB, 2(4) cores(hyperthreads)*.

Так что *абсолютные* результаты поменялись относительно прошлого раза,
при этом их *соотношение* осталось примерно тем же. Но теперь я могу
**доверять** результатам.

Домашний ноут, кстати, теперь показывает совсем скромные результаты в
multiprocessing (а вроде бы те же 4 ядра, но он быстро перегревается и
уходит в **throttling**:

    (aiozmq)andrew@tiktaalik2:~/projects/aiozmq (master)$ python benchmarks/simple.py -n 5000 -t 100
     Run tests for 100*5000 iterations: ['aiozmq.rpc', 'core aiozmq', 'single thread raw zmq', 'single thread zmq with poller', 'zmq with threads']
....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................

    RPS calculated as 95% confidence interval
    Results for aiozmq.rpc
    RPS: 975: [747, 1204],	mean: 1038.325 μs,	standard deviation 115.027 μs

    Results for core aiozmq
    RPS: 3136: [2176, 4097],	mean: 326.753 μs,	standard deviation 52.423 μs

    Results for single thread raw zmq
    RPS: 6429: [4138, 8721],	mean: 161.251 μs,	standard deviation 32.622 μs

    Results for single thread zmq with poller
    RPS: 5461: [3879, 7042],	mean: 187.659 μs,	standard deviation 31.816 μs

    Results for zmq with threads
    RPS: 6349: [4730, 7967],	mean: 160.318 μs,	standard deviation 22.218 μs


## Мораль

Сравнивайте **относительные показатели** -- современные процессоры
слишком сильно отличаются друг от друга.  *Ноутбук* не допускает
перегрева, *десктоп* к нагреванию относится помягче а *сервер* живет в
раскалённом состоянии. Виртуальные сервера на Амазоне -- отдельная тема.

Если делаете benchmarks -- проверьте результаты **статистически**.

Распространенный в интернете способ: "запустить три раза и выбрать
лучший результат" никуда не годится.

Средний результат тоже ни о чём не говорит.

Нужно запустить тест как минимум 33 (а лучше 333) раза и оценить не
только **среднее время** а и **среднеквадратичное отклонение** как минимум.

Я -- далёкий от математики человек. Как и подавляющее большинство
программистов, мне кажется. Изучал в институте, да с тех пор прошло 15
лет и почти всё выветрилось.

*Среднеквадратичное отклонение* мне мало о чём говорит. Много или мало
-- сложно было сказать сразу после того как я начал собирать
статистику для своих тестов.

Но я вспомнил что на моём любимом сайте
[http://elementy.ru](http://elementy.ru/) в научных статьях часто
используют схемки вроде этой:

![Доверительный
интервал](http://upload.wikimedia.org/wikipedia/commons/d/df/Confidenceinterval.png).

*К сожалению сайт **элементов** стал недоступен из Киева по
неизвестной мне причине несколько недель назад. Читаю его через
буржуйский VPN.
Поэтому буду рассказывать что к чему на примере из википедии.*

Столбики -- это средние значения. А красные вертикальные
линии на столбиках -- это именно *доверительные интервалы*, заданные с
какой-то погрешностью. Обычно принято использовать 95% если явно не
указано другое значение.

По этим красным вертикальным линиям очень наглядно видно, насколько
полученное **среднее значение** *измеряемой величины* **размазано по выборке**.

Надеюсь, все поняли **основную идею**: *смотрим на результат теста* и
обращаем внимание на **границы изменчивости измеряемой
величины**.

## P.S.

Мне бы тоже стоило выложить похожую картинку, но я не знаю
[matplotlib](http://matplotlib.org/) и не могу сделать красивый
график.

Помогите, пожалуйста.

Лучший вариант -- *pull request* на
[github](https://github.com/aio-libs/aiozmq) который позволит
запустить ``python benchmarks/simple.py -n 5000 -t
100 --save-plot=file.png`` или что-то вроде того.

# Upd

Спасибо [Артему
Дудареву](https://plus.google.com/108114175145093679567), теперь у
меня есть хорошая генерилка графиков:

![картинка](https://dl.dropboxusercontent.com/u/63019062/benchmarks/aiozmq/aoizmq.png)
